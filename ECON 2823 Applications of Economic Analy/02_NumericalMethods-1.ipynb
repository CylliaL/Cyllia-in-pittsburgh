{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Methods\n",
    "___\n",
    "Adding some tools to our toolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Aim of this unit will be to help us understand how we go from thinking about things fully theoretically, to making them numeric.\n",
    "\n",
    "* In this way we'll lose some generality, as we won't be able to think about things abstractly\n",
    "* But we'll also gain a whole bunch of concrete detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In general, as part of my process, I tend to always start out with fully analytical models:\n",
    "* This is where you figure out the moving parts\n",
    "* What are inputs/outputs\n",
    "* What's the objective here, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But at some point the model either gets too complicated to solve, or I'm trying move the model to data to estimate things concretely, at which point we move from having a theoretical model on a piece of paper, to trying to get this model to engage with numbers\n",
    "\n",
    "Today we'll go through ways to mirror some of the analysis skills we've used in the theory sections, but within the computer.\n",
    "\n",
    "To do this, we'll switch from analytical representations, into numerical, and detail what we gain from this, and what we need to be concerned with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Methods\n",
    "Our aims is to go over some three key numerical tools:\n",
    "1. Numerical derivatives\n",
    "2. Numerical solutions to equations\n",
    "3. Numerical Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I'm going to go a little out of order though in that I want to do one thing on solving equations first..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Equations\n",
    "Suppose that we have a simple system of equations:\n",
    "$$ \\begin{array}{rcl} 2 x_1+ 3 x_2 & = & 6 \\\\  x_1+ x_2 & = & 2 \\end{array}$$\n",
    "\n",
    "You probably solved this type of equation in high school... \n",
    "\n",
    "...and you can probably figure this one out almost in your head\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So the unique solution to this is:\n",
    "$$ \\begin{array}{rcl} x_1 & = & 0 \\\\  x_2 & = & 2 \\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While we probably didn't need it, we could have represented that equation as a linear system:\n",
    "$$\\begin{array}{rcl} 2 x_1+ 3 x_2 & = & 6 \\\\  x_1+ x_2 & = & 2 \\end{array} \\Rightarrow \\left[ \\begin{array}{cc}\n",
    " 2 & 3 \\\\ \n",
    " 1 & 1 \\end{array}\\right] \n",
    " \\left(\\begin{array}{c}x_1\\\\x_2\\end{array}\\right)= \\left(\\begin{array}{c}6\\\\2\\end{array}\\right)\n",
    " $$\n",
    " \n",
    "So the linear equation system is $\\mathbf{A}\\mathbf{x}=\\mathbf{c}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So this such linear systems can easily be solved by using matrix inverses:\n",
    "$$\\left(\\begin{array}{c}x_1\\\\x_2\\end{array}\\right)= \\mathbf{A}^{-1} \\mathbf{c}$$\n",
    "\n",
    "* Each row of $\\mathbf{A}$ and $\\mathbf{c}$  represents a particular equation. \n",
    "* Each column in $\\mathbf{A}$ represents the contributed effect from each of the underlying variables in $\\mathbf{x}$, the things we're trying to solve for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "A <- rbind( c(2,3),c(1,1) )\n",
    "c <- rbind( c(6),c(2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "solve(A)%*%c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While this example we could have done more quickly by hand, once the number of equations becomes large, linear algebra becomes very useful.\n",
    "\n",
    "Implicitly, you've been using the benefits of this in the last semseter, because you were trying to minimize the total sum of squares for a regression model\n",
    "$$ \\sum_i\\left(y_{i}-(\\beta_0+\\beta_1 x_{i,1}+\\ldots +\\beta_k x_{i,k})\\right)^2 $$\n",
    "\n",
    "Taking first-order conditions to try to get each term minimized led to $k+1$ equations in the $k+1$ unknowns (the $\\beta$ terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's possible to make the above simpler to represent $\\mathbf{y}$ and $\\boldsymbol{\\beta}$ as vectors, and $\\mathbf{X}$ as the $n\\times (k+1)$ data matrix.\n",
    "\n",
    "At the end of the day though, the first-order conditions imply a system of linear equations:\n",
    "$$\\mathbf{A} \\hat{\\boldsymbol{\\beta}}=\\mathbf{c}$$\n",
    "(where the additional structure told us that $\\mathbf{A}=\\mathbf{X}^T\\mathbf{X}$ and $\\mathbf{c}=\\mathbf{X}^T\\mathbf{y}$)\n",
    "\n",
    "The benefit of being able to solve linear equations is that we got a nice analytical expression for the estimates:\n",
    "$$ \\hat{\\boldsymbol{\\beta} }= (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The point of this is:\n",
    "* Some brief review of linear algebra\n",
    "* Solving linear equations is mechanical so long as you've got the right numbers of unknowns and equations\n",
    "* In the OLS model where we optimized to **minimize** the sum of squares, the output formula was easy to work with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Methods\n",
    "So, having gone over a very simple set of equations, let's come back to option 1 here.\n",
    "1. Numerical derivatives\n",
    "2. Numerical solutions to equations\n",
    "3. Numerical Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Analytical Derivatives\n",
    "### Easy Ones\n",
    "Sometimes it is trivial for us to take take a derivative, for example:\n",
    "$$ f(x)=a x^2+b x+c$$\n",
    "where we can say that:\n",
    "$$ f^\\prime(x)=2 a x+b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Medium Ones\n",
    "Othertimes, we can probably solves something, it just takes us a bit longer:\n",
    "$$ g(x)=(a x^2+b x+c)\\cdot e^{-\\lambda x}$$\n",
    "where we can say that:\n",
    "$$ f^\\prime(x)=-\\lambda e^{-\\lambda x}(a x^2 +bx +c)+e^{-\\lambda x}(2ax+b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hurt-Your-Head Ones\n",
    "And still other times, it's just not worth really thinking about the analytical formula...\n",
    "\n",
    "For me this is basically anything with a triple product, or any type of harmonic term, but to make things concrete, consider:\n",
    "\n",
    "$$ f(x)=\\sqrt{x^2 e^{\\sin(x)}+\\Phi(x \\cdot \\log(x+1))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's not to say that this can't be solved, where the analytical derivative is:\n",
    "$$ f^\\prime(x)=\\frac{-\\frac{x e^{-\\frac{1}{2} x^2 \\log ^2(x+1)} \\log (x+1) (x+(x+1) \\log (x+1))}{\\sqrt{2 \\pi } (x+1)}+x^2 e^{\\sin (x)} \\cos (x)+2 x e^{\\sin (x)}}{2 \\sqrt{\\frac{e^{-\\frac{1}{2} x^2 \\log ^2(x+1)}}{\\sqrt{2 \\pi }}+x^2\n",
    "   e^{\\sin (x)}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The issue is that the amount of time we might dedicate to solving this one experession likely isn't worth it. Instead, we'll just go back to the definition of a derivative, and try and get a numerical approximation to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In fact, I just solved this with Mathematica (a symbolic algebra packagae). The point here is that the amount of time you should dedicate into getting an analytical expression is only as good as what you'll use it for...\n",
    "\n",
    "* Would knowing the formula for $f^\\prime(x)$ save me time?\n",
    "* Would it be more accurate?\n",
    "* Would it give be a better understanding into the nature of the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graidents & Derivatives\n",
    "### Discrete changes\n",
    "A lot of the time when we're thinking about changes, it's usually a discrete quantity change $$x\\mapsto x+\\Delta x$$ \n",
    "and we're interested in the ensuing effect on another variable $$y=f(x)\\mapsto f(x+\\Delta x)=f(x)+\\Delta y.$$\n",
    "\n",
    "We can think of the underlying gradient as the ratio of the movements $$ \\frac{\\Delta y}{\\Delta x}=\\frac{f(x+\\Delta x)-f(x)}{\\Delta x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Derivatives\n",
    "As we make the change in $x$ smaller and smaller, this gradient approaches the derivative:\n",
    "$$\\tfrac{d}{dx}y= \\lim_{\\Delta x\\rightarrow 0} \\frac{f(x+\\Delta x)-f(x)}{\\Delta x}$$\n",
    "The derivative tells us the ratio of the changes when the input change is infinitesimally small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rather than the exact limit, we can therefore try to numerically approximate the derivative by a using a very small value of the input change $\\Delta x$. \n",
    "\n",
    "* That is, we will choose $\\Delta x$ to be suitably small number $\\epsilon$\n",
    "\n",
    "* However, we can't make $\\epsilon$ too small, as we can get into problems with how computers store numbers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coding it up\n",
    "Let's go back to out simplest example $f(x)=a x^2+b x+c$,\n",
    "but where we make the problem exact by setting $a=2$, $b=-10$ and $c=3$\n",
    "\n",
    "So:\n",
    "* $f(x)=2 x^2- 10 x+3$\n",
    "* $ f^\\prime(x)=4 x-10$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's code up an example, of  out derivative approximation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy.f(1)\n",
    "d.easy.f(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a numerical derivative function in R\n",
    "num.deriv <- function(f, x, eps ) { \n",
    "    # this function  has THREE inputs to it:\n",
    "    # (function f, point of derivative x, small change eps)\n",
    "    (f(x+eps)-f(x))/eps\n",
    "}\n",
    "# Define the function we're interested in taking derivatives of\n",
    "easy.f<- function(x) {\n",
    "    2*x**2-10*x+3\n",
    "}\n",
    "# Define the actual derivative\n",
    "d.easy.f<- function(x) {\n",
    "    4*x-10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Take Numerical Derivatives!\n",
    "eps <- 1e-5\n",
    "# Use the function we created\n",
    "nd <- num.deriv(easy.f,3,1e-6)\n",
    "ad <- d.easy.f(3)\n",
    "err <- nd-ad\n",
    "# Output it as three separate columns\n",
    "rbind(nd,ad,err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Another Example\n",
    "Let's go back to our medium-hard example where we'll just take the product of our previous quadratic function with an exponential function (that decays here)\n",
    "\n",
    "$$\\left(2 x^2- 10 x+3 \\right)e^{-x/10}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the medium-hard function as an R *function*\n",
    "med.f<-function(x) { \n",
    " easy.f(x)*exp(-x/10)\n",
    "}\n",
    "# Analytical derivative\n",
    "d.med.f<-function(x) {\n",
    "    -0.1*exp(-0.1*x)*easy.f(x)+exp(-0.1*x)*(2*2*x-10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Take Numerical Derivatives!\n",
    "eps <- 1e-5\n",
    "nd <- num.deriv(med.f,3,eps)\n",
    "ad <- d.med.f(3)\n",
    "err <- nd-ad\n",
    "rbind(nd,ad,err) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Define some colors for graphing\n",
    "I took these from the [Pitt Brand Identity website](https://www.brand.pitt.edu/visual-identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Pitt.Blue <- \"#003594\"\n",
    "Pitt.Gold <- \"#FFB81C\"\n",
    "Pitt.DGray <- \"#75787B\"\n",
    "Pitt.Gray <- \"#97999B\"\n",
    "Pitt.LGray <- \"#C8C9C7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Draw a graph of the function in R\n",
    "(I'm going to draw a few graphs, so I'm going to define the base plot environment first )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "library(\"ggplot2\")\n",
    "library(\"repr\") # here i'm just loading a package to format the output\n",
    "options(repr.plot.width=10, repr.plot.height=10/1.68)\n",
    "base <- ggplot() +aes()+ theme( panel.background = element_rect(fill = \"white\", size = 0.5, linetype = \"solid\"),\n",
    "  panel.grid.major = element_line(size = 0.5, linetype = 'solid', colour =Pitt.Gray), \n",
    "  panel.grid.minor = element_line(size = 0.25, linetype = 'solid', colour = \"white\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "base + geom_function(fun = med.f,colour=Pitt.Blue,  size=1)+  xlim(0, 100) +\n",
    "ggtitle(\"Function Graph\") + xlab('x') +ylab('f(x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a numerical derivative for every x from 0 to 100\n",
    "x.range <- c(0:100)\n",
    "nd.med.f<- num.deriv( med.f,x.range,1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Draw the graphs:\n",
    "base+ geom_function(fun = d.med.f, colour=Pitt.Blue, size=1)+ # Draw the analytical derivative\n",
    "ggtitle(\"Derivative Graph\") + xlab('x') +ylab('f(x)')+ xlim(0, 100) +\n",
    "# Layer the numerical approximations on top as points!\n",
    "geom_point(color=Pitt.Gold,size=3,aes(y=nd.med.f,x=x.range) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We can graph the errors (the difference) to check they are small:\n",
    "nd.med.f.error<- nd.med.f - d.med.f(x.range) # numerical - actual\n",
    "base+ aes(x=x.range,y=nd.med.f.error)+geom_point(size=3,color=Pitt.Gold)+xlim(0, 100) +\n",
    "ggtitle(\"Errors\") + xlab('x') +ylab('Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## What about higher-order derivatives ?\n",
    "\n",
    "Suppose that we want to get a numerical version of the second derivative $\\tfrac{d^2}{dx^2}f(x)$.\n",
    "\n",
    "One way would be to just define this via the iterated procedure, where the second derivative is just the derivative of the derivative..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So we want to try and approximate: \n",
    "$$\\tfrac{d^2}{dx^2}f(x)=\\tfrac{d}{dx}f^\\prime(x)= \\lim_{\\Delta x\\rightarrow 0} \\frac{f^\\prime(x+\\Delta x)-f^\\prime(x)}{\\Delta x}$$\n",
    "through a small increment $\\epsilon$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And we could plug in  numerical approximation for the derivative at $x$:\n",
    "$$f^\\prime_\\epsilon(x)= \\frac{f(x+\\epsilon)-f(x)}{\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "and at $x$\n",
    "$$f^\\prime_\\epsilon(x+\\epsilon)= \\frac{f(x+2\\epsilon)-f(x+\\epsilon)}{\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One approximation for the second-derivative would then be to look at how the numerical slope of $f(x)$ changes across the increment $\\epsilon$:\n",
    "$$f^{\\prime\\prime}_\\epsilon(x)= \\frac{f^\\prime(x+\\epsilon)-f^\\prime(x)}{\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Where if you went through the prior formulas you'd find that this approximation is actually:\n",
    "$$f^{\\prime\\prime}_\\epsilon(x)=\\frac{f(x+2\\epsilon)-2 f(x+\\epsilon)+f(x)}{\\epsilon^2 }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Coding this up as a formula we get \n",
    "num.deriv.2 <- function(f, x, eps ) {\n",
    "    ( f(x+2*eps)-2*f(x+eps) +  f(x)  )/(eps**2)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thinking back to our easy formula as a test for this, what's the second derivative of:\n",
    "$$ 2 x^2- 10 x+3 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num.deriv.2(easy.f,2,1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How can we generate formulas for these approximations?\n",
    "___\n",
    "## Are there better, more accurate formulas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One way to think through this procedure is to consider the **Taylor expansion** of the function around the point $x$ for any deviation $\\Delta x$\n",
    "$$f(x+\\Delta x)=f(x)+\\frac{f^\\prime(x)}{1!}\\Delta x+\\frac{f^{\\prime\\prime}(x)}{2!}\\Delta x^2+\\frac{f^{\\prime\\prime\\prime}(x)}{3!}\\Delta x^3+\\ldots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Taylor Expansion\n",
    "$$f(x+\\Delta x)=f(x)+\\frac{f^\\prime(x)}{1!}\\Delta x+\\frac{f^{\\prime\\prime}(x)}{2!}\\Delta x^2+\\frac{f^{\\prime\\prime\\prime}(x)}{3!}\\Delta x^3+\\ldots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let $\\Delta x$ be our small increment $\\epsilon$ in the numerical derivative formula, raising $\\epsilon$ to increasing powers makes each of the successive terms even smaller,\n",
    "$$f(x+\\epsilon)=f(x)+\\frac{f^\\prime(x)}{1!}\\epsilon+\\frac{f^{\\prime\\prime}(x)}{2!}\\epsilon^2+\\frac{f^{\\prime\\prime\\prime}(x)}{3!}\\epsilon^3+\\ldots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So our formula for the derivative was giving us:\n",
    "$$\\frac{f(x+\\epsilon)-f(x)}{\\epsilon}=f^\\prime(x) + \\frac{f^{\\prime\\prime}(x)}{2!}\\epsilon + \\frac{f^{\\prime\\prime\\prime}(x)}{3!}\\epsilon^2+\\ldots $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which if $\\epsilon$ is really small relative to the higher order derivatives is:\n",
    "$$\\frac{f(x+\\epsilon)-f(x)}{\\epsilon}= f^\\prime(x) + O(\\epsilon)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Similarly, let's apply this reasoning to our formula for the second derivative.\n",
    "$$\\frac{f(x+2\\epsilon)-2 f(x+\\epsilon)+f(x)}{\\epsilon^2 } $$\n",
    "so we need to figure out what the $f(x+2\\epsilon)$ terms is from the Taylor expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$f(x+2\\epsilon)=f(x)+2 f^\\prime(x)\\epsilon +2f^{\\prime\\prime}(x)\\epsilon^2+8\\frac{f^{\\prime\\prime\\prime}(x)}{6}\\epsilon^3+\\ldots$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The second-derivative is therefore giving us:\n",
    "$$\\frac{f(x+2\\epsilon)-2 f(x+\\epsilon)+f(x)}{\\epsilon^2 }= f^{\\prime\\prime}(x)+\\tfrac{7}{6}f^{\\prime\\prime\\prime}(x)\\epsilon +\\ldots=f^{\\prime\\prime}(x)+O(\\epsilon)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Taylor Expansion\n",
    "The same thinking on the Taylor expansion, can help us think through potentially better approximations.\n",
    "\\begin{aligned}\n",
    "f(x+\\epsilon) &=f(x)+f^\\prime(x)\\epsilon+f^{\\prime\\prime}(x)\\frac{\\epsilon^2}{2}+f^{\\prime\\prime\\prime}(x)\\frac{\\epsilon^3}{3!}+\\ldots \\\\\n",
    "f(x)&=f(x) \\\\\n",
    "f(x-\\epsilon) &=f(x)-f^\\prime(x)\\epsilon+f^{\\prime\\prime}(x)\\frac{\\epsilon^2}{2}-f^{\\prime\\prime\\prime}(x)\\frac{\\epsilon^3}{3!}+\\ldots\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If we use both a forward and a backward increment to  generate what is called a *centered difference* equation we get:\n",
    "$$\\frac{f(x+\\epsilon)-f(x-\\epsilon)}{2\\epsilon}=f^\\prime(x)+O(\\epsilon^2)$$\n",
    "Where this is a more accurate approximation of the derivative at $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Define a better numerical derivative \n",
    "c.num.deriv <- function(f, x, eps ) {\n",
    "    (f(x+eps)-f(x-eps))/(2*eps)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the centered derivative for our medium function\n",
    "c.nd.med.f<- c.num.deriv(med.f,x.range,1e-4)\n",
    "nd.med.f<- num.deriv(med.f,x.range,1e-4)\n",
    "# Define the difference from the true derivative\n",
    "c.nd.med.f.error<-c.nd.med.f-d.med.f(x.range)\n",
    "nd.med.f.error<-nd.med.f-d.med.f(x.range)\n",
    "# Draw the graph \n",
    "base+\n",
    "geom_point(size=2.5,aes(x=x.range,y=c.nd.med.f.error),color=Pitt.Gold)+ # Gold for centered\n",
    "geom_point(size=2.5,color=Pitt.Blue,aes(x=x.range,y=nd.med.f.error))+ # Blue for original method\n",
    "xlim(0, 100) +ggtitle(\"Errors\") + xlab('x') +ylab('df(x)/dx-(Derivative Approximation)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Taylor Expansion\n",
    "$$f(x+\\Delta x)=f(x)+\\frac{f^\\prime(x)}{1!}\\Delta x+\\frac{f^{\\prime\\prime}(x)}{2!}\\Delta x^2+\\frac{f^{\\prime\\prime\\prime}(x)}{3!}\\Delta x^3+\\ldots$$\n",
    "So long as $\\Delta x$ is small, raising it to successive powers becomes even smaller.\n",
    "\n",
    "We can use this formula to write out any equations for a fixed multiple of a step-size $\\epsilon$\n",
    "\n",
    "Consider doing this for a double difference on either side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ \\mathbf{f}=\\left( \\begin{array}{c} f(x+2\\epsilon) \\\\ f(x+\\epsilon) \\\\ f(x) \\\\ f(x-\\epsilon) \\\\f(x-2\\epsilon) \\end{array}\\right)\n",
    "= \\left( \\begin{array}{c}\n",
    "f(x)+2\\frac{f^\\prime(x)}{1!}\\epsilon+4\\frac{f^{\\prime\\prime}(x)}{2!}\\epsilon^2+8\\frac{f^{\\prime\\prime\\prime}(x)}{3!}\\epsilon^3+\\ldots \\\\\n",
    "f(x)+\\frac{f^\\prime(x)}{1!}\\epsilon+\\frac{f^{\\prime\\prime}(x)}{2!}\\epsilon^2+\\frac{f^{\\prime\\prime\\prime}(x)}{3!}\\epsilon^3+\\ldots \\\\\n",
    "f(x) \\\\\n",
    "f(x)-\\frac{f^\\prime(x)}{1!}\\epsilon+\\frac{f^{\\prime\\prime}(x)}{2!}\\epsilon^2-\\frac{f^{\\prime\\prime\\prime}(x)}{3!}\\epsilon^3+\\ldots\\\\\n",
    "f(x)-2\\frac{f^\\prime(x)}{1!}\\epsilon+4\\frac{f^{\\prime\\prime}(x)}{2!}\\epsilon^2-8\\frac{f^{\\prime\\prime\\prime}(x)}{3!}\\epsilon^3+\\ldots \\end{array} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Figuring out the formulas\n",
    "But, if we think about the underlying variables creatively, this is just solving a linear system of equations|\n",
    "\n",
    "Let's call $f(x)$ the variable $D_0 f$, $f^\\prime(x)\\epsilon$ the variable $D_1 f$ etc, and set this up as a linear system.\n",
    "\n",
    "So our first equation is:\n",
    "$$f(x+2\\epsilon)=D_0f+\\tfrac{2}{1!} D_1f+\\tfrac{2^2}{2!} D_2f+\\tfrac{2^3}{3!}D_3f \\tfrac{2^4}{4!}D_4f +O(\\epsilon^5)$$\n",
    "our second equation is:\n",
    "$$f(x+\\epsilon)=D_0f+\\tfrac{1}{1!} D_1f+\\tfrac{1}{2!} D_2f+\\tfrac{1}{3!}D_3f +\\tfrac{1}{4!}D_4f +O(\\epsilon^5)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## In Matrix notation\n",
    "If we're willing to ignore terms of $\\epsilon^5$ (assuming they're very small) then:\n",
    " $$\\left( \\begin{array}{c} f(x+2\\epsilon) \\\\ f(x+\\epsilon) \\\\ f(x) \\\\ f(x-\\epsilon) \\\\f(x-2\\epsilon) \\end{array}\\right)\\simeq \\left[ \\begin{array}{ccccc}\n",
    " 1 & 2 & 2 & \\tfrac{4}{3} & \\tfrac{4}{6} \\\\ \n",
    " 1 & 1 & \\tfrac{1}{2} & \\tfrac{1}{6} & \\tfrac{1}{24} \\\\ \n",
    " 1 & 0 & 0 & 0 & 0 \\\\\n",
    " 1 & -1 & \\tfrac{1}{2} & -\\tfrac{1}{6} & \\tfrac{1}{24} \\\\\n",
    " 1 & -2 & 2 & -\\tfrac{4}{3} & \\tfrac{4}{6} \\end{array}\\right] \n",
    " \\left(\\begin{array}{c}D_0f\\\\ D_1f\\\\ D_2f \\\\ D_3f\\\\ D_4f\\end{array}\\right)\n",
    " $$\n",
    " Which is a linear system $\\mathbf{f}=\\mathbf{A}\\mathbf{d}$.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "A=matrix(1:35, nrow = 5, ncol = 5)\n",
    "for(i in -2 : 2) { # Here I'm jsut coding this up with a For Loop\n",
    "    A[3-i,1]=1 # This just makes sure 2 is in the 1 slot\n",
    "    for (j in 2:5) {\n",
    "        A[3-i,j]= (i**(j-1))/factorial(j-1)\n",
    "    }\n",
    "}\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " Our aim is to find some simple weight vector $\\mathbf{w}$ for each of the terms in $\\mathbf{f}$ such that it gives us a good approximation for one of the terms in $\\mathbf{d}$, where we will choose which derivative we are after (i.e first, second)\n",
    " \n",
    "So, supposing that we want to find some combination of the $\\mathbf{f}$ terms which isolates the first derivative term ($D_1 f=f^\\prime(x)\\epsilon$).\n",
    " \n",
    "We can therefore think of this as being the task of trying to find a vector of weights $\\mathbf{w}$ (one for each term in $\\mathbf{f}$) such that the result gives us a given combination of the derivatives (through another chosen vector $\\mathbf{b}$)\n",
    "$\\mathbf{w}^T \\mathbf{f}=\\mathbf{w}^T\\mathbf{A}\\mathbf{d} =\\mathbf{b}^T\\mathbf{d}$ for\n",
    "$$\\mathbf{b}=\\left(\\begin{array}{c}0\\\\1\\\\0\\\\0\\\\0\\end{array}\\right)$$\n",
    "so that means our weighting function must be given by:\n",
    "$$\\mathbf{w}^T A= \\mathbf{b}^T \\Rightarrow \\mathbf{w}^T=\\mathbf{b}^T\\mathbf{A}^{-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "b=matrix(0, nrow = 5, ncol = 1) # ininitialize to all zeros as a column vector\n",
    "Deriv=1 \n",
    "b[Deriv+1,1]=1 # Place a 1 in the Deriv-th row\n",
    "Ai=solve(A) # Take the matrix inverse of the transoise\n",
    "w1=t(b) %*% Ai\n",
    "colnames(w1)<-c(\"f(x+2d)\",\"f(x+d)\",\"f(x)\",\"f(x-d)\",\"f(x-2d)\")\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "b=matrix(0, nrow = 5, ncol = 1)\n",
    "Deriv=2\n",
    "b[Deriv+1,1]=1\n",
    "Ai=solve(A) # Take the matrix inverse of the transoise\n",
    "w2=t(b) %*% Ai \n",
    "colnames(w2)<-c(\"f(x+2d)\",\"f(x+d)\",\"f(x)\",\"f(x-d)\",\"f(x-2d)\")\n",
    "w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, if we multiply all this together we get:\n",
    "$$ w_1 f(x+2\\epsilon )+ w_2 f(x+\\epsilon )+ w_3 f(x ) + w_4 f(x-\\epsilon ) + w_4 f(x-2\\epsilon ) \\simeq D_1 f = f^\\prime (x)\\epsilon $$\n",
    "\n",
    "And therefore we can create an approximation to the derivative $f^\\prime (x)$ via:\n",
    "$$f^\\prime (x)\\simeq \\frac{w_1 f(x+2\\epsilon )+ w_2 f(x+\\epsilon )+ w_3 f(x ) + w_4 f(x-\\epsilon ) + w_4 f(x-2\\epsilon )}{\\epsilon} $$\n",
    "where the error is of the Order $O(\\epsilon^4)$ (as we divided by $\\epsilon$ here!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n.deriv.1<-  function(f,x,eps) {\n",
    "    # So this will be an O(e^4) method\n",
    "    (w1[1]*f(x+2*eps)+w1[2]*f(x+eps)+w1[3]*f(x)+w1[4]*f(x-eps)+w1[5]*f(x-2*eps))/eps  \n",
    "}\n",
    "n.deriv.1(med.f,2,1e-2)\n",
    "n.deriv.1(med.f,2,1e-5)\n",
    "\n",
    "n.deriv.1(sin,0.2,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n.deriv.2<-  function(f,x,eps) {\n",
    "    # This will be an O(e^3) method\n",
    "    (w2[1]*f(x+2*eps)+w2[2]*f(x+eps)+w2[3]*f(x)+w2[4]*f(x-eps)+w2[5]*f(x-2*eps))/eps**2\n",
    "}\n",
    "n.deriv.2(med.f,2,1e-4)\n",
    "n.deriv.2(med.f,2,1e-2)\n",
    "n.deriv.2(sin,0.2,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can graph the errors as before for the `med.f` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "high.nd.med.f<- n.deriv.1(med.f,x.range,1e-4)\n",
    "high.nd.med.f.error<-high.nd.med.f-d.med.f(x.range)\n",
    "base+geom_point(color=Pitt.Blue,aes(x=x.range,y=c.nd.med.f.error))+ # Centered formula from before\n",
    "geom_point(color=Pitt.Gold,aes(x=x.range,y=high.nd.med.f.error) )+ # The formula from our linear system\n",
    "xlim(0, 100) +\n",
    "ggtitle(\"Errors\") + xlab('x') +ylab('df(x)/dx-(Derivative Approximation)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that we could have changed $\\mathbf{b}$ to pick out an different $D_k f$  (though we need to be careful about what we need to divide by to get the final derivative )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Solutions to an Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-linear Equations\n",
    "\n",
    "Linear equations have well-defined solutions.\n",
    "* We know *if* we can solve a system based on the rank of the matrix\n",
    "* If it's solvable, there's a unique solution\n",
    "* And we know the exact formula for this solution, through the inverse of the matrix\n",
    "\n",
    "Non-linear equations are tougher "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While some non-linear equations do have well-defined solutions, if we can invert the non-linear part:\n",
    "$$ x^4= 12\\Rightarrow x = 12^{1/4} $$\n",
    "others are not easily invertible\n",
    "$$ x\\cdot e^{x^2+3x}= 3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In addition, we already know that non-linear equations can admit multiple solutions!\n",
    "\n",
    "Consider the quadratic formula. \n",
    "$$ a x^2 +bx +c =0$$\n",
    "While we do have a closed form solution here, except in very special cases, there are two roots to the equation (which may or may not have real values!) from the formula:\n",
    "$$ x= \\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our aim here will be to come up with simple ways to generate numeric solutions to these non-linear problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numerical Solution\n",
    "There are lots of more advanced methods, but let's focus on building some intuition for how numerical solution works.\n",
    "\n",
    "The core-method here is illustrated by the Newton-Rhapson iteration to try to solve the root of a continuous function:\n",
    "$f(x)=0$\n",
    "\n",
    "Method Procedure:\n",
    "* Step 1: Guess an initial solution $x_0$\n",
    "* Step 2: Figure out the value of the function at this point $f(x_0)$\n",
    "* Step 3: Figure out the derivative of the funcation here $f^\\prime(x_0)$\n",
    "\n",
    "We then output a new guess\n",
    "$$ x_1= x_0 - \\dfrac{f(x_0)}{f^\\prime(x_0)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But then we can use that new guess as an input to find another:\n",
    "$$ x_2= x_1 - \\dfrac{f(x_1)}{f^\\prime(x_1)}.$$\n",
    "And so on, so the induction is that:\n",
    "$$ x_{n+1}= x_n - \\dfrac{f(x_n)}{f^\\prime(x_n)},$$\n",
    "and you stop whenever $|x_{n+1}-x_n|$ is small enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "As an example, let's consider the function:\n",
    "$$ f(x)= 2 x^2 - 10 x + 3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Graphing this:\n",
    "![Image](https://alistairjwilson.github.io/MQE_AW/i/nr_1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So we have $ f(x)= 2 x^2 - 10 x + 3 $, and because this is an easy function we know that $ f^\\prime(x)= 4 x - 10$\n",
    "\n",
    "Method Procedure:\n",
    "* Step 1: Guess an initial solution $x_0=4$\n",
    "* Step 2: Figure out the value of the function at this point $f(4)=-5$\n",
    "* Step 3: Figure out the derivative of the funcation here $f^\\prime(4)=6$\n",
    "\n",
    "We then output a new guess\n",
    "$$ x_1= x_0 - \\dfrac{f(x_0)}{f^\\prime(x_0)} = 4-\\dfrac{(-5)}{6}=\\frac{29}{6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1 and 2:\n",
    "![Image](https://alistairjwilson.github.io/MQE_AW/i/nr_2.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Zooming in\n",
    "![Image](https://alistairjwilson.github.io/MQE_AW/i/nr_3.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Finding the derivative\n",
    "![Image](https://alistairjwilson.github.io/MQE_AW/i/nr_4.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Putting it all together to get $x_1$\n",
    "![Image](https://alistairjwilson.github.io/MQE_AW/i/nr_5.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Figure out $f(x_1)$\n",
    "![Image](https://alistairjwilson.github.io/MQE_AW/i/nr_6.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Figure out $f^\\prime(x_1)$ to get $x_2$\n",
    "![Image](https://alistairjwilson.github.io/MQE_AW/i/nr_7.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "newton.rhapson<-function(f,x0,tol=1e-8,eps=1e-6) { # input is a function, a start val, and optional tolerance/scale\n",
    "    xx=x0  # initial value \n",
    "    fval=f(xx) \n",
    "    Error=abs(f(xx)) # how far away from zero is it!\n",
    "    ii=1 # here we'll generate a counter for the number of steps\n",
    "    # Repeat the steps until the error is less than the tolerance\n",
    "    while (Error>tol){\n",
    "        fd=n.deriv.1(f,xx,eps) # take the numeric derivative using our prev formula\n",
    "        xx=xx-fval/fd # Newton-Rhapson iteration from formula\n",
    "        fval=f(xx) \n",
    "        Error=abs(fval) # How far from zero now!\n",
    "        ii=ii+1\n",
    "    } \n",
    "    print(paste(toString(ii),\" steps\"))\n",
    "    xx\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Actual values from quadratic formula\n",
    "actual.roots <- c( (10-sqrt(100-4*2*3))/4, (10+sqrt(100-4*2*3))/4) \n",
    "names(actual.roots)<-c(\"Low Root\",\"High Root\")\n",
    "actual.roots\n",
    "# Run our approximation\n",
    "newton.rhapson(easy.f,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "actual.roots\n",
    "# Run it with a higher tolerance\n",
    "newton.rhapson(easy.f,4,tol=1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What happens if we change the starting value?\n",
    "![Image](https://alistairjwilson.github.io/MQE_AW/i/nr_1.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "actual.roots\n",
    "newton.rhapson(easy.f,1,tol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Going back to our *medium* function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "base+ geom_function(fun = med.f, colour=Pitt.Blue, size=1)+ \n",
    "ggtitle(\"Function Graph\") + xlab('x') +ylab('f(x)')+xlim(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "newton.rhapson(med.f ,7.5,tol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "newton.rhapson(med.f,5,tol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We also know that  $\\left(2 x^2- 10 x+3 \\right)\\cdot e^{-x/10}=0$ in the limit as $x\\rightarrow\\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "base+ geom_function(fun = med.f, colour=Pitt.Blue, size=1)+ \n",
    "ggtitle(\"Function Graph\") + xlab('x') +ylab('f(x)')+xlim(10, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Define a new version of the function that terminates after a certain number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "newton.rhapson<-function(f,x0,tol=1e-8,eps=1e-6,maxiter=50) {\n",
    "    xx=x0  # initial value \n",
    "    fval=f(xx)\n",
    "    Error=abs(f(xx))  # how far away from zero is it!\n",
    "    ii=1 # iterator\n",
    "    # Repeat the steps until the error is less than the tolerance, or we've done too many steps\n",
    "    while (Error>tol & ii <maxiter ){\n",
    "        fd=n.deriv.1(f,xx,eps)\n",
    "        xx=xx-fval/fd # newton-rhapson iterator\n",
    "        fval=f(xx)\n",
    "        Error=abs(fval)\n",
    "        ii=ii+1\n",
    "        if (i>10) print(i)\n",
    "    } \n",
    "    if (ii>=maxiter) print(\"Exited due to non-convergence\") # print a warning if too many steps \n",
    "    print(paste(toString(ii),\" steps\"))\n",
    "    xx\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton.rhapson(med.f,90,tol=1e-32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med.f.2<-function(x) med.f(x)+1\n",
    "base+ geom_function(fun = med.f.2, colour=Pitt.Blue, size=1)+ \n",
    "ggtitle(\"Function Graph\") + xlab('x') +ylab('f(x)')+xlim(10, 415)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med.f.2(1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton.rhapson(med.f.2,1 , 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton.rhapson<-function(f,x0,tol=1e-8,eps=1e-6,maxiter=50) {\n",
    "    xx=x0  # initial value \n",
    "    fval=f(xx)\n",
    "    Error=abs(f(xx))  # how far away from zero is it!\n",
    "    ii=1 # iterator\n",
    "    nan.catch=0\n",
    "    # Repeat the steps until the error is less than the tolerance, or we've done too many steps\n",
    "    while (Error>tol & ii <maxiter &  nan.catch==0){\n",
    "        fd=n.deriv.1(f,xx,eps)\n",
    "        print(c(xx,fd))\n",
    "        if (is.nan(fd)){\n",
    "            nan.catch=1\n",
    "            print(\"Derivative not defined\")\n",
    "        }\n",
    "        else {\n",
    "            xx=xx-fval/fd # newton-rhapson iterator\n",
    "            fval=f(xx)\n",
    "            Error=abs(fval)\n",
    "            ii=ii+1\n",
    "            }\n",
    "    } \n",
    "    if (ii>=maxiter) print(\"Exited due to non-convergence\") # print a warning if too many steps \n",
    "    print(paste(toString(ii),\" steps\"))\n",
    "    xx\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "newton.rhapson(med.f,90,tol=1e-8, maxiter=10)\n",
    "newton.rhapson(med.f,90,tol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Obviously, we shouldn't try to reinvent the wheel here. There are many other solvers which use other clever methods to find roots.\n",
    "\n",
    "However, sometimes we need to know what we're doing when we use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#install.packages('rootSolve')\n",
    "library(rootSolve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "multiroot(med.f.2, start = 1:10,rtol=1e-12,atol=1e-8)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "python",
   "pygments_lexer": "r",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
