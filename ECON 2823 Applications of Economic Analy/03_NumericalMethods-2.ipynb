{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interested-norfolk",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Methods 2 - Optimization\n",
    "___\n",
    "Continuing on..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-physics",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As we're continuing from last time, let me just redefine and run some of the equations from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-maldives",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Define colors\n",
    "Pitt.Blue<-\"#003594\"\n",
    "Pitt.Gold<-\"#FFB81C\"\n",
    "Pitt.DGray <- \"#75787B\"\n",
    "Pitt.Gray <- \"#97999B\"\n",
    "Pitt.LGray <- \"#C8C9C7\"\n",
    "# ggplot preferences\n",
    "library(\"ggplot2\")\n",
    "library(\"repr\")\n",
    "options(repr.plot.width=10, repr.plot.height=10/1.68)\n",
    "base<- ggplot() +aes()+ theme( panel.background = element_rect(fill = \"white\", linewidth = 0.5, linetype = \"solid\"),\n",
    "  panel.grid.major = element_line(linewidth = 0.5, linetype = 'solid', colour =Pitt.Gray), \n",
    "  panel.grid.minor = element_line(linewidth = 0.25, linetype = 'solid', colour = \"white\")\n",
    "  )\n",
    "#Find out the weights for our numerical solver for first derivative\n",
    "# First Order derivative function\n",
    "A=matrix(1:35, nrow = 5, ncol = 5)\n",
    "for(i in -2 : 2) {\n",
    "    A[3-i,1]=1 # This just makes sure 2 is in the 1 slot\n",
    "    for (j in 2:5) {\n",
    "        A[3-i,j]= (i**(j-1))/factorial(j-1)\n",
    "    }\n",
    "}\n",
    "b=matrix(0, nrow = 5, ncol = 1)\n",
    "Deriv=1\n",
    "b[Deriv+1,1]=1\n",
    "Ai=solve(A) # Take the matrix inverse of the transoise\n",
    "w1=t(b) %*% Ai\n",
    "colnames(w1)<-c(\"f(x+2d)\",\"f(x+d)\",\"f(x)\",\"f(x+d)\",\"f(x-2d)\")\n",
    "w1\n",
    "w1*12\n",
    "StudentList<-read.csv('../Students.csv')\n",
    "selstudent<-function(n) {\n",
    "    sel<-data.frame(sample(StudentList$Student,n,replace=FALSE))\n",
    "    names(sel)<-\"Selection\"\n",
    "    sel\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-memorial",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Second-order derivative function\n",
    "b=matrix(0, nrow = 1, ncol = 5)\n",
    "Deriv=2\n",
    "b[1,Deriv+1]=1\n",
    "Ai=solve(A) # Take the matrix inverse of the transoise\n",
    "w2=b %*% Ai\n",
    "colnames(w2)<-c(\"f(x+2d)\",\"f(x+d)\",\"f(x)\",\"f(x+d)\",\"f(x-2d)\")\n",
    "w2\n",
    "12*w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-viewer",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# These are the centered methods, so they're an order more accurate.\n",
    "# So this will be an O(e^4) method\n",
    "n.deriv.1<-  function(f,x,eps=1e-6) (-f(x+2*eps)/12+8*f(x+eps)/12-8*f(x-eps)/12+f(x-2*eps)/12)/eps\n",
    "# This will be an O(e^3) method (dividing by eps^2!)\n",
    "n.deriv.2<-  function(f,x,eps=1e-3)  (-f(x+2*eps)/12+16*f(x+eps)/12-30*f(x)/12+16*f(x-eps)/12-1*f(x-2*eps)/12)/eps**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-entity",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numerical Methods\n",
    "So, having gone over a very simple set of equations, let's come back to option 1 here.\n",
    "- [x] Numerical derivatives\n",
    "- [x] Numerical solutions to equations\n",
    "- [ ] Numerical Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-trance",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numerical Optimization\n",
    "\n",
    "We're going to show how numerical optimization works within \n",
    " - one dimension\n",
    " - with a continuous function \n",
    "\n",
    "While I'll talk about the extensions to multiple dimensions for the derivative methods we'll talk about, there are many other methods, and lots of lots of details., I'll give you a quick overview of some of the other methods, why we might need them,  and the terminology.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-cliff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If the problem is smooth (and this generalizes to multiple dimensions) then we can try to use the same **first-order conditions** that you've used in the theoretical parts of the course.\n",
    "\n",
    "That is, a necessary condition to be at an optimal point for a continous differentiable function is that the slope is zero:\n",
    "$$ f^\\prime(x)=0$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-criticism",
   "metadata": {},
   "source": [
    "![Image](https://alistairjwilson.github.io/MQE_AW/i/OptimalSlope.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-minister",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Option 1\n",
    "Just code up the analytical derivative, and solve for its roots! As we will see, in some situations, this will be possible, but in others an analytical solution will be a bear, in which case we'll opt for **Option 2** below.\n",
    "\n",
    "But, for the examples we looked at before, this is definitely a possibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-virtue",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Define the analytical derivatives of $ f(x)= 2 x^2 - 10 x + 3 $ and $g(x)=f(x)e^{-x/10}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-support",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "f <- function(x) 2*x**2-10*x+3\n",
    "g <-function(x) f(x)*exp(-x/10)\n",
    "d.f <- function(x) 4*x-10\n",
    "d.g <-function(x) -0.1*exp(-0.1*x)*f(x)+exp(-0.1*x)*(2*2*x-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-insert",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And so we can use the previous Newton-Rhapson routine, but where we'll change the input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "newton.rhapson<-function(func.in,x0,tol=1e-8,eps=1e-6,output=FALSE) {\n",
    "    xx=x0  # initial value \n",
    "    fval=func.in(xx) \n",
    "    Error=abs(func.in(xx)) # how far away from zero is it!\n",
    "    ii=1 # here we'll generate a counter for the number of steps\n",
    "    # Repeat the steps until the error is less than the tolerance\n",
    "    while (Error>tol){\n",
    "        fd=n.deriv.1(func.in,xx,eps) # take the numeric derivative using our prev formula\n",
    "        xx=xx-fval/fd # Newton-Rhapson iteration from formula\n",
    "        fval=func.in(xx) \n",
    "        Error=abs(fval) # How far from zero now!\n",
    "        ii=ii+1\n",
    "    } \n",
    "    if (output) print(paste(\"Converged to solution \",  toString(round(xx,digits=5)),\" in \",toString(ii-1),\" steps\"))\n",
    "    xx\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-station",
   "metadata": {},
   "source": [
    "*Note: this is still using a numerical derivative function. You could get rid of that too if you had an analytical expression for the the second derivative function too.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-graduation",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So let's look for the optimal solutions to our easy function $ f(x)= 2 x^2 - 10 x + 3 $ by looking for a solution to $ f^\\prime(x)= 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "base+ geom_hline(yintercept=0,color=Pitt.Gold,linewidth=1)+geom_function(fun = d.f, colour=Pitt.Blue, linewidth=1)+ \n",
    "xlab('x') +ylab(\"f'(x)\")+xlim(0,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-encyclopedia",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So this clearly has only one solution (which is easy to get from $f^\\prime(x)=4x-10=0$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.sol<-newton.rhapson(d.f,1,output=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-radio",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And for the optimal solutions to our medium function $ g(x)= \\left(2 x^2 - 10 x + 3 \\right)e^{-x/10}$, looking for roots to $g^\\prime(x)=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "base+ geom_hline(yintercept=0,color=Pitt.Gold,linewidth=1)+geom_function(fun = d.g, colour=Pitt.Blue, linewidth=1)+ \n",
    "xlab('x') +ylab(\"g'(x)\") +xlim(0,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-shopping",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So this has at least two solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "g.sol.1<-newton.rhapson(d.g,0, output=TRUE)\n",
    "g.sol.2<-newton.rhapson(d.g,20, output=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-rotation",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Option 2\n",
    "Suppose we don't want to even think about the derivatives, then we can just go fully numeric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-cabinet",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "newton.rhapson.fullnumeric<-function(f,x0,tol=1e-8,eps=1e-6,maxiter=50,output=FALSE) {\n",
    "    # argumens: f: function to optimize, x0: initial guess\n",
    "    xx=x0\n",
    "    #This is our equation for the derivative\n",
    "    fval=n.deriv.1(f,xx,eps)\n",
    "    Error=abs(fval)\n",
    "    ii=1\n",
    "    # Repeat the steps until the error is less than the tolerance\n",
    "    while (Error>tol & ii <maxiter ){\n",
    "        # this is our equation for the second derivative\n",
    "        fd=n.deriv.2( f , xx, sqrt(eps) )\n",
    "        # So this is the Newton-rhapson step for\n",
    "        # solving the derivative root\n",
    "        # via x1=x0-f'(x0)/f\"(x0)\n",
    "        xx=xx-fval/fd\n",
    "        fval=n.deriv.1(f,xx,eps)\n",
    "        Error=abs(fval)\n",
    "        ii=ii+1\n",
    "    } \n",
    "    if (ii>=maxiter) print(\"Exited due to non-convergence\")        \n",
    "    if (output && ii<maxiter ) print(paste(\"Converged to solution \",  toString(round(xx,digits=5)),\" in \", toString(ii-1),\" steps\"))\n",
    "    xx\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-pixel",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the easy function:\n",
    "$$ f(x)= 2 x^2 - 10 x + 3 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-particular",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "f.sol.nd<-newton.rhapson.fullnumeric(f,x0=1,output=TRUE)\n",
    "# Differences?\n",
    "fsol<-f.sol.nd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-telling",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the medium function:\n",
    "$$ g(x)= \\left(2 x^2 - 10 x + 3 \\right)e^{-x/10}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-depression",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "g.sol.1.nd <- newton.rhapson.fullnumeric(g,0,output=TRUE)\n",
    "g.sol.2.nd <- newton.rhapson.fullnumeric(g,20,output=TRUE)\n",
    "# Differences?\n",
    "c(g.sol.1 -g.sol.1.nd,  g.sol.2-g.sol.2.nd )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-correlation",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So identical solutions to the version where we calculated the derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-crowd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But what type of solutions are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-jacksonville",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![Image](https://alistairjwilson.github.io/MQE_AW/i/OptimalSlope.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-pollution",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Both local minima and local maxima satistfy the $f^\\prime(x)=0$ condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-greek",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Type of optima \n",
    "While all of the solutions satisfy $f^\\prime(x)=0$ we can figure out whether it's a local minima or maxima by how the slopw changes around this point.\n",
    "* For maxima the slope must be:\n",
    "    - increasing to the  right\n",
    "    - decreasing to the left\n",
    "* For minima the slope must be:\n",
    "    - decreasing to the right\n",
    "    - increasing to the left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-demand",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So this implies that:\n",
    " * For a maxima our second derivative estimate should be negative\n",
    "     - slope decreasing over $x$, $f^{\\prime\\prime}(x)<0$\n",
    " * For a minima our second derivative estimate should be positive\n",
    "     - slope increasing over $x$, $f^{\\prime\\prime}(x)>0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-nurse",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So we can check the 2nd derivative at each solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-highlight",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Check soc for solution 1\n",
    "#g.sol.1\n",
    "n.deriv.2( g , g.sol.1 )\n",
    "# Check soc for solution 2\n",
    "#g.sol.2\n",
    "n.deriv.2( g , g.sol.2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-brake",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So the first solution is a local minima, the second solution a local maxima. As this is in one dimension, easy to check it with a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-dylan",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "base+ geom_hline(yintercept=g(g.sol.1), color=Pitt.Gold,linewidth=1)+\n",
    "geom_hline(yintercept=g(g.sol.2), color=Pitt.Gold,linewidth=1)+\n",
    "geom_function(fun = g, colour=Pitt.Blue, linewidth=2)+\n",
    "geom_point(aes(x=g.sol.1,y=g(g.sol.1)),size=5,color=\"black\")+\n",
    "geom_point(aes(x=g.sol.2,y=g(g.sol.2)),size=5,color=\"black\")+\n",
    "ggtitle(\"Illustrating solutions\") + xlab('x') +ylab('g(x)')+xlim(g.sol.1-5,g.sol.2+5)\n",
    "g(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-nutrition",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Global vs Local optima\n",
    "However, only one of these is a **global** optima. Which one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-interaction",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If we're considering any value of $x\\in\\mathbb{R}$ the the maxima  is not a global max as we can find values of $x$ where g(x) is larger!\n",
    "    - For very negative values of $x$ the function beomes unboundely large.\n",
    "    - If we narrowed the range for $x$ to be $x\\geq0$ then it would be global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-consent",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The local minima we identified is also a global minimizer of the function\n",
    "    - For very large values of $x$ the value of the function tends to zero from above.\n",
    "    - The value at the second solutions is negative\n",
    "\n",
    "Can see this if we zoom out on the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-arena",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "base+ geom_hline(yintercept=g(g.sol.1), color=Pitt.Gold,linewidth=1)+\n",
    "geom_hline(yintercept=g(g.sol.2), color=Pitt.Gold,linewidth=1)+\n",
    "geom_hline(yintercept=0, color=Pitt.Gold,linewidth=1,linetype=\"dashed\")+\n",
    "geom_function(fun = g, colour=Pitt.Blue,linewidth=2)+\n",
    "geom_point(aes(x=g.sol.1,y=g(g.sol.1)),size=5,color=\"black\")+\n",
    "geom_point(aes(x=g.sol.2,y=g(g.sol.2)),size=5,color=\"black\")+\n",
    "ggtitle(\"Local vs. Global\") + xlab('x') +ylab('g(x)')+xlim(-5,105)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-intention",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local solutions\n",
    "When we're minimizing an objective function that is not single-peaked/troughed, then our methods can sometimes get stuck at local solutions, and not find our way to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-nickel",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wavy.f<- function(x)  (-(x-2)**2)*cos((x-2))-90*cos(20*(x-2))*exp(-30*(x-2)**4)+ exp((x-2)**2)\n",
    "base+ggtitle(\"Illustrating solutions\") + xlab('x') +ylab('g(x)')+xlim(0,4)+\n",
    "geom_function(fun = wavy.f, colour=Pitt.Blue, linewidth=2, n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-husband",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "newton.rhapson.fullnumeric(wavy.f,0,output=TRUE)\n",
    "newton.rhapson.fullnumeric(wavy.f,5,output=TRUE)\n",
    "newton.rhapson.fullnumeric(wavy.f,8,output=TRUE)\n",
    "newton.rhapson.fullnumeric(wavy.f,2.3,output=TRUE)\n",
    "newton.rhapson.fullnumeric(wavy.f,2.05,output=TRUE)\n",
    "newton.rhapson.fullnumeric(wavy.f,2.1,output=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-saint",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More dimensions\n",
    "So the equations we've been optimizing so far have been with a single input. How does this all generalize to more dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-carroll",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As an example, consider:\n",
    "$$ f(x_1,x_2)=2 x_1^2+2 x_2^2-2 x_1 x_2 - 5x_1+x_2$$\n",
    "This type of function is still pretty easy for us to solve analytically, where the first-order conditions are:\n",
    "$$\\begin{array}{rccl} \n",
    "\\frac{\\partial f(x_1,x_2)}{\\partial x_1} & = & 4 x_1- 2 x_2 -5&=0 \\\\\n",
    "\\frac{\\partial f(x_1,x_2)}{\\partial x_2} & = & 4 x_2- 2 x_1 +1 &=0 \\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-locking",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This can be solved to find the unique minimizing solution here: $x_1^\\star=\\tfrac{3}{2}$ and $x_2^\\star=\\tfrac{1}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-pressure",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# define a function of 2 inputs\n",
    "f2 <- function(x) (2* x[1]^2 +2*x[2]^2-2*x[1]*x[2]-5*x[1]+x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-charles",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f2( c(0,0) )\n",
    "f2( c(1.5,0.5) )\n",
    "f2( c(1.6,0.4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-despite",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ f(x_1,x_2)=2 x_1^2+2 x_2^2-2 x_1 x_2 - 5x_1+x_2$$\n",
    "\n",
    "![Image](https://alistairjwilson.github.io/MQE_AW/i/MultiObj.png)\n",
    "\n",
    "Can we minimize this function numerically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-snake",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The answer is yes. There are a few more details (and a bit more multi-variate calculus). In particular, we need to define: \n",
    "* the gradient vector of partial derivatives  $\\nabla f=\\left(\\frac{\\partial f}{\\partial x_1}, \\ldots ,\\frac{\\partial f}{\\partial x_n}\\right)^T$\n",
    "* The matrix of second-order derivatives called the **Hessian** matrix: $$\\mathbf{H_f}(x) =\\left[ \\begin{array}{cccc} \n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} &\\frac{\\partial^2 f}{\\partial x_1\\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1\\partial x_n} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2^2} &\\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_n\\partial x_1} &\\frac{\\partial^2 f}{\\partial x_n\\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n",
    "\\end{array}\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-content",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "I'm not going to get into all of the details, but the numerical methods work similarly to before.\n",
    "\n",
    "The analog to our second-order condition with one variable relates to a property of the Hessian matrix $\\mathbf{H}$ assessed at each optima, whether it is: \n",
    "* Positive definite: for every  non-zero vector $\\mathbf{z}$ we have $\\mathbf{z}^T \\mathbf{H} \\mathbf{z}>0$ then we have a local **minimum**\n",
    "* Negative definite: for every  non-zero vector $\\mathbf{z}$ we have $\\mathbf{z}^T \\mathbf{H} \\mathbf{z}<0$ then we have a local **maximum**\n",
    "\n",
    "These conditions are actually easy to compute numerically as there's a relationship with the eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-textbook",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization in R\n",
    "R has several optimization routines already coded, and I'm not going to go into the algorithms for each to much. However, a simple gradient based method is implemented here using the `optim()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-abraham",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f2.optim<-optim(c(0,0) ,f2, method = \"BFGS\", hessian=TRUE)\n",
    "f2.optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-jason",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's look at the returned optimal value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-cartoon",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "names(f2.optim$par)<- c(\"Optimal x1\",\"Optimal x2\")\n",
    "f2.optim$par "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-workstation",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So this is really close to the exact optimal!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-judgment",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can also check that this is a minimizer of the function via the optional returned Hessian matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install.packages('matrixcalc')\n",
    "library(matrixcalc)\n",
    "is.positive.definite(f2.optim$hessian)\n",
    "is.negative.definite(f2.optim$hessian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-environment",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `optim()` function minimizes whichever function you provide it. As such, if you're after a maximizer, you should multiply your function by -1!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-florida",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Finding local values\n",
    "As we saw, when the function has lots of minima, it can be difficult for us to find the right solution using gradient methods.\n",
    "\n",
    "This doesn't go away with more dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-breed",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define a two dimensional version of our wavy function from before\n",
    "wavy.f.dim2 <- function(x) (wavy.f(x[1])+90)*(1+(x[2]-3)**2)/100\n",
    "wavy.sol<-optim(c(0,0),wavy.f.dim2,method = \"BFGS\")\n",
    "# This is the actual global minimum of the function\n",
    "wavy.f.dim2(c(2,3)) \n",
    "\n",
    "# This is the function value we get using the gradient based method:\n",
    "wavy.f.dim2(wavy.sol$par)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-facing",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-gradient methods\n",
    "There are also several other methods available, some of which use gradients, some of which do not.\n",
    "\n",
    "In fact, the default method (called Nelder-Mead) does not use gradients at all. Instead, it creates a mesh that's one dimension bigger than the function it's trying to optimize. A really nice visualization of the Nelder-Mead algorithm is given [here](https://www.benfrederickson.com/numerical-optimization/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-murder",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "wavy2.sol.nm <- optim( c(0,0), wavy.f.dim2 )\n",
    "wavy2.sol.nm$par\n",
    "wavy.f.dim2(wavy2.sol.nm$par)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-authority",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nelder-Mead can do poorly though on high-dimensional problems (see the city-pair examination at the end of the [visualization link](https://www.benfrederickson.com/numerical-optimization/)), or where there is some flatness in the objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-reply",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Nelder-Mead, also performs quite poorly on one-dimensional problems. In fact, R will warn you if you try to use it there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optim(12,wavy.f)\n",
    "# it wants you to use: optim(3,wavy.f, method=\"Brent\", lower=-10,upper=20)\n",
    "optim(3,wavy.f, method=\"L-BFGS-B\", lower=-10,upper=20)\n",
    "wavy.f(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-resource",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In fact, for one-dimensional problems on a fixed interval, you can use a different function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize(g,interval=c(-10,5 )) \n",
    "optimize(wavy.f,interval=c(1.5,2.5)) \n",
    "wavy.f(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-waters",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simulated Annealing\n",
    "One available method that does get used a fair amount when we're very concerned about reaching global maxima/minima is called simulated annealing.\n",
    "* This is a stochastic method, where by the computer makes a series of random calls to the function\n",
    "* Can see a partial visualization of this in this [YouTube video](https://www.youtube.com/watch?v=iaq_Fpr4KZc)\n",
    "* More sophisticated versions of this incorporating selection dynamics in the random points also exist (genetic algorithms)\n",
    "* These optimization methods can be more time-consuming though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-lancaster",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# On our simple function \n",
    "optim(c(0,0),wavy.f.dim2, method=\"SANN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-scholarship",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# On our very wavy function\n",
    "optim(0,wavy.f, method=\"SANN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-investor",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can also combine the simulated-annealing approach with some gradient-based methods.\n",
    "\n",
    "Here I'll do it for the final value, though you could imagine doing this at intermediate steps too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol1.sann<-optim(c(0,0),wavy.f.dim2, method=\"SANN\")\n",
    "optim(sol1.sann$par,wavy.f.dim2, method=\"BFGS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-feelings",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are variants of the random approach in simulated annealing but with an evolutionary approach to selection in a population of random points too, where these are called genetic algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-vehicle",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-uniqueness\n",
    "\n",
    "Sometimes our objective does not have a unique maximizer\n",
    "\n",
    "Consider the function:\n",
    "$$ f(x_1,x_2)=-(x_1+x_2-3)^2$$\n",
    "\n",
    "Any value of $(x_1,x_2)$ that sums to 3 will minimize it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-specific",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This relate to a problem of identification when we come back to estimation:\n",
    "\n",
    "Suppose $x_1$ is *ability* and $x_2$ is *effort*. Together they generate *output* given by $x_1+x_2$\n",
    "* The idea then is that $(x^\\star_1+x^\\star_2)=\\text{output}=3$ could be identified in our data\n",
    "* However, whether that output is coming from ability or effort, we cannot discern"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R 4.3",
   "language": "R",
   "name": "ir43"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
